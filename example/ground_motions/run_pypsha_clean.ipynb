{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to run pypsha and get output pgas\n",
    "##### From Neetesh Sharma and modified by Emily Mongold and Neetesh Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from NNR import NNR\n",
    "\n",
    "import pickle\n",
    "import utm\n",
    "import numpy.matlib as nm\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following only for the first run of pypsha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pypsha(site_file,nmaps,outdir):\n",
    "    # have some fixed parameters, such as attenuations and PGA as IM\n",
    "    test_site = psha.PSHASite(name = 'site',\n",
    "                            site_filename = site_file,\n",
    "                            erf=1, intensity_measures = [1],\n",
    "                            attenuations = [1,2,4],\n",
    "                            overwrite=True)\n",
    "    test_site.write_opensha_input(overwrite = True)\n",
    "    test_site.run_opensha(overwrite= True, write_output_tofile = True)\n",
    "    event_set = psha.PshaEventSet(test_site)\n",
    "    sa_intensity_ids = [\"ASK2014_PGA\",\"BSSA2014_PGA\",\"CY2014_PGA\"]\n",
    "    event_set.generate_sa_maps(sa_intensity_ids, nmaps)\n",
    "    with open(outdir + 'event_save_all_alameda.pickle','wb') as handle:\n",
    "        pickle.dump(event_set, handle)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"all_alameda_sites.csv\"\n",
    "site_file = os.path.join(os.getcwd(),file)\n",
    "run_pypsha(site_file,2,'./')\n",
    "with open('./event_save_all_alameda.pickle','rb') as handle:\n",
    "    event_set = pickle.load(handle)\n",
    "test_site = psha.PSHASite(name = 'alameda',\n",
    "                            site_filename = site_file,\n",
    "                            erf=1,\n",
    "                            intensity_measures = [1],\n",
    "                            attenuations = [1,2,4],\n",
    "                            overwrite=True)\n",
    "test_site.write_opensha_input(overwrite = True)\n",
    "test_site.run_opensha(overwrite= True, write_output_tofile = True)\n",
    "event_set = psha.PshaEventSet(test_site)\n",
    "nmaps = 2 #per erf row\n",
    "sa_intensity_ids = [\"ASK2014_PGA\",\"BSSA2014_PGA\",\"CY2014_PGA\"]\n",
    "\n",
    "event_set.generate_sa_maps(sa_intensity_ids, nmaps)\n",
    "with open('./event_save_all_alameda.pickle','wb') as handle:\n",
    "    pickle.dump(event_set, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run starting here once event_save.pickle has been generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsim = event_set.maps['ASK2014_PGA'].shape[0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_file = \"./all_alameda_sites.csv\"\n",
    "with open('./event_save_all_alameda.pickle','rb') as handle:\n",
    "    event_set = pickle.load(handle)\n",
    "\n",
    "\n",
    "# generate random gmm from 0-2 for each event\n",
    "gmm = np.array(np.random.choice([0, 1, 2], size=(nsim,)))\n",
    "np.savetxt('gmm.csv', gmm, delimiter=\",\")\n",
    "\n",
    "# once gmm is generated, load from here:\n",
    "# gmm = np.loadtxt('gmm.csv',delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to save pgas_pypsha.csv for the new vs30 values\n",
    "def run_pypsha_pgas(savedir,event_file,gmm):\n",
    "    '''function to obtain pgas from pypsha output file based on gmm for each scenario.\n",
    "    savedir is the directory where the output will be saved\n",
    "    event_file is the im directory + 'event_save.pickle' unless event_file has a new name\n",
    "    gmm is the random variable for the ground motion model for each scenario\n",
    "    output is the text is saved in the im directory as 'pgas_pypsha.csv'\n",
    "    '''\n",
    "    \n",
    "    with open(event_file,'rb') as handle:\n",
    "        event_set = pickle.load(handle)\n",
    "    \n",
    "    sa_intensity_ids = [\"ASK2014_PGA\",\"BSSA2014_PGA\",\"CY2014_PGA\"] # should edit to obtain automatically\n",
    "    ask_events = event_set.maps[sa_intensity_ids[0]]\n",
    "    ask_events = ask_events.reset_index(level='map_id')\n",
    "    ask_events = ask_events[ask_events['map_id'] == 0]\n",
    "    ask_events.drop('map_id', axis=1,inplace=True)\n",
    "    \n",
    "    pga = np.zeros(ask_events.shape)\n",
    "    n = 0\n",
    "    for scen in event_set.maps[sa_intensity_ids[0]]['site0'].keys():\n",
    "        if scen[-1] == 0:\n",
    "            mod = int(gmm[n])\n",
    "            pga[n,:] = event_set.maps[sa_intensity_ids[mod]].loc[scen]\n",
    "            n += 1\n",
    "            \n",
    "    np.savetxt(savedir + 'pypsha_pgas.csv',pga,delimiter=',')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pypsha_pgas('./',event_file='./event_save_all_alameda.pickle',gmm=gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to get pgas at each grid location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgas = {}\n",
    "pgaZ = np.genfromtxt('./pypsha_pgas.csv', delimiter=',')\n",
    "data = pd.read_csv(site_file,index_col=0)\n",
    "lons = data['x'] # or lon\n",
    "lats = data['y'] # or lat\n",
    "\n",
    "nsim = len(pgaZ)\n",
    "\n",
    "utmX = np.zeros(len(lons))\n",
    "utmY = np.zeros(len(lons))\n",
    "for i in range(len(utmX)):\n",
    "    (utmX[i], utmY[i], reg,northrn) = utm.from_latlon(lats[i],lons[i])\n",
    "\n",
    "X = (utmX - 558500)/1000  ## changed from 559000 [cut off lhs island]\n",
    "Y = (utmY - 4172400)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 2423)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = range(117)\n",
    "x = nm.repmat(xs,1,114)\n",
    "indX = list(x[0])\n",
    "\n",
    "indY = []\n",
    "y = []\n",
    "for i in range(114):\n",
    "    y.append(np.full(117,i))\n",
    "for i in range(len(y)):\n",
    "    for j in list(y[i]):\n",
    "        indY.append(j)\n",
    "        \n",
    "y_t = list(map(lambda x:x/10,indY))\n",
    "x_t = list(map(lambda x:x/10,indX))\n",
    "utmX = list(map(lambda x:(1000*x) + 558500,x_t))\n",
    "utmY = list(map(lambda x:(1000*x) + 4172400,y_t))\n",
    "\n",
    "grid = pd.DataFrame(columns = ['y','x'])\n",
    "grid['y'] = y_t\n",
    "grid['x'] = x_t\n",
    "LAT = np.zeros(len(utmX))\n",
    "LON = np.zeros(len(utmY))\n",
    "for i in range(len(utmX)):\n",
    "    lat,lon = utm.to_latlon(utmX[i], utmY[i], reg, northrn)\n",
    "    LAT[i] = lat\n",
    "    LON[i] = lon\n",
    "\n",
    "\n",
    "grid['lat'] = LAT\n",
    "grid['lon'] = LON\n",
    "grid['indX'] = indX\n",
    "grid['indY'] = indY\n",
    "\n",
    "np.array(pgaZ.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsim = pgaZ.shape[0]\n",
    "\n",
    "Alameda = gpd.read_file('../alameda_plots/alameda_county.geojson')\n",
    "for sim in range(nsim):\n",
    "    Z = []\n",
    "    for i in range(pgaZ.shape[1]): #pga at each location\n",
    "        Z.append(pgaZ[sim,i])\n",
    "    Z = np.array(Z)\n",
    "    z_t = NNR(np.array([x_t,y_t]).T,np.array([X,Y]).T,Z,sample_size = -1, n_neighbors = 4, weight = 'distance2')\n",
    "    grid['pga'] = z_t\n",
    "    gdf = gpd.GeoDataFrame(grid, geometry=gpd.points_from_xy(grid['lon'], grid['lat']),crs = 'EPSG:4326')\n",
    "    points = gpd.sjoin(gdf,Alameda)\n",
    "    points.reset_index(inplace=True,drop = True)\n",
    "\n",
    "    if sim == 0:\n",
    "        for i in range(len(points)):\n",
    "            pgas[i] = []\n",
    "    for i in range(len(points)):\n",
    "        pgas[i].append(points['pga'][i])\n",
    "        \n",
    "for i in range(len(points)):\n",
    "    pgas[i] = np.array(pgas[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pgas).to_csv('./pgas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN JUST THIS CODE TO SAVE M TO CSV\n",
    "with open('./event_save_all_alameda.pickle','rb') as handle:\n",
    "    event_set = pickle.load(handle)\n",
    "M = np.array(event_set.events.metadata['magnitude'])\n",
    "np.savetxt('M.csv',M,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN JUST THE CPT POINTS AND SAVE THAT AS PGAS_CPT.CSV TO AVOID KEY ERROR\n",
    "\n",
    "def setup_cpt(datadir):\n",
    "    ''' function setup_cpt to load USGS cpt data from folder and output a dictionary\n",
    "    input: datadir is the directory path to the cpt data\n",
    "    output: cpt is a dictionary with an np directory for each borehole set of cpt data\n",
    "    '''\n",
    "\n",
    "    d = {}\n",
    "    names = []\n",
    "    cpt = {}\n",
    "    # Constants\n",
    "    g = 9.81\n",
    "    Pa = 0.101325  # MPa\n",
    "    rho_w = 1  # Mg/m^3\n",
    "    gamma_w = rho_w * g / 1000  # MPa\n",
    "\n",
    "    for filename in filter(lambda x: x[-4:] == '.txt', os.listdir(datadir)):\n",
    "\n",
    "        with open(os.path.join(datadir, filename)) as f:\n",
    "            name = datadir + filename\n",
    "            df_temp = pd.read_csv(name, delimiter=\"\\s+\", skiprows=17)\n",
    "            df_temp = df_temp.dropna(axis='columns', how='all')\n",
    "            df_temp.columns = ['Depth', 'Tip_Resistance', 'Sleeve_Friction', 'Inclination', 'Swave_travel_time']\n",
    "            df_temp = df_temp[-((df_temp['Sleeve_Friction'] < 0) | (df_temp['Tip_Resistance'] < 0))]\n",
    "            df_temp = df_temp.reset_index(drop=True)\n",
    "            df_temp = df_temp[df_temp['Depth'] <= 20]\n",
    "            df_temp['Sleeve_Friction'] = df_temp['Sleeve_Friction'] / 1000  # convert to units of MPa\n",
    "\n",
    "            temp = pd.DataFrame(np.zeros(shape=(len(df_temp), 7)),\n",
    "                                columns=['start', 'q_c', 'f_s', 'd', 'dz', 'gamma', 'R_f'])\n",
    "            temp['q_c'][0] = df_temp['Tip_Resistance'][0] / 2\n",
    "            temp['f_s'][0] = df_temp['Sleeve_Friction'][0] / 2\n",
    "            temp['d'][0] = np.average([temp['start'][0], df_temp['Depth'][0]])\n",
    "            temp['dz'][0] = df_temp['Depth'][0] - temp['start'][0]\n",
    "            temp['R_f'][0] = 100 * temp['f_s'][0] / temp['q_c'][0]\n",
    "            temp['gamma'][0] = gamma_w * (0.27 * (np.log10(temp['R_f'][0])) +\n",
    "                                          0.36 * np.log10(temp['q_c'][0] / Pa) + 1.236)\n",
    "            for i in range(1, len(df_temp)):\n",
    "                temp['start'][i] = df_temp['Depth'].iloc[i - 1]\n",
    "                temp['f_s'][i] = np.average([df_temp['Sleeve_Friction'].iloc[i], df_temp['Sleeve_Friction'].iloc[i - 1]])\n",
    "                temp['q_c'][i] = np.average([df_temp['Tip_Resistance'].iloc[i], df_temp['Tip_Resistance'].iloc[i - 1]])\n",
    "                temp['d'][i] = np.average([temp['start'][i], df_temp['Depth'].iloc[i]])\n",
    "                temp['dz'][i] = df_temp['Depth'].iloc[i] - temp['start'][i]\n",
    "                temp['R_f'][i] = 100 * temp['f_s'][i] / temp['q_c'][i]\n",
    "                # Calculating soil unit weight from Robertson and Cabal (2010)\n",
    "                if temp['R_f'][i] == 0:\n",
    "                    if temp['q_c'][i] == 0:\n",
    "                        temp['gamma'][i] = gamma_w * 1.236\n",
    "                    else:\n",
    "                        temp['gamma'][i] = gamma_w * (0.36 * np.log10(temp['q_c'][i] / Pa) + 1.236)\n",
    "                elif temp['q_c'][i] == 0:\n",
    "                    temp['gamma'][i] = gamma_w * (0.27 * (np.log10(temp['R_f'][i])) + 1.236)\n",
    "                else:\n",
    "                    temp['gamma'][i] = gamma_w * (0.27 * np.log10(temp['R_f'][i]) +\n",
    "                                                  0.36 * np.log10(temp['q_c'][i] / Pa) + 1.236)\n",
    "\n",
    "            temp['dsig_v'] = temp['dz'] * temp['gamma']\n",
    "\n",
    "            key = list(dict(l.strip().rsplit(maxsplit=1) for l in open(name) if any(l.strip().startswith(i) for i in 'File name:')).values())[0]\n",
    "            names.append(key)\n",
    "            d[key] = dict(l.strip().rsplit('\\t', maxsplit=1) for l in open(name) \\\n",
    "                          if (any(l.strip().startswith(i) for i in ('\"UTM-X', '\"UTM-Y', 'Date')) and len(l.strip().rsplit('\\t', maxsplit=1)) == 2))\n",
    "\n",
    "            cpt[key] = {}\n",
    "            cpt[key]['CPT_data'] = temp\n",
    "\n",
    "            for i in d[key]:\n",
    "                if i.startswith('\"UTM-X'):\n",
    "                    cpt[key]['UTM-X'] = int(d[key][i])\n",
    "                elif i.startswith('\"UTM-Y'):\n",
    "                    cpt[key]['UTM-Y'] = int(d[key][i])\n",
    "\n",
    "                elif i.startswith('Date'):\n",
    "                    cpt[key]['Date'] = datetime.datetime.strptime(d[key][i], '%m/%d/%Y')\n",
    "\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        cpt[names[i]]['Lat'], cpt[names[i]]['Lon'] = utm.to_latlon(cpt[names[i]]['UTM-X'], cpt[names[i]]['UTM-Y'], 10,\n",
    "                                                                   northern=True)\n",
    "    return cpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = setup_cpt('./USGS_CPT_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(cpt.keys())\n",
    "lats = []\n",
    "lons = []\n",
    "for key in cpt:\n",
    "    lats.append(cpt[key]['Lat'])\n",
    "    lons.append(cpt[key]['Lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_file = \"./all_alameda_sites.csv\"\n",
    "with open('./event_save_all_alameda.pickle','rb') as handle:\n",
    "    event_set = pickle.load(handle)\n",
    "    pgas = {}\n",
    "pgaZ = np.genfromtxt('./pypsha_pgas.csv', delimiter=',')\n",
    "data = pd.read_csv(site_file,index_col=0)\n",
    "X = data['x'] # or lon\n",
    "Y = data['y'] # or lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsim = len(pgaZ)\n",
    "grid = pd.DataFrame(columns = ['pga','lat','lon'])\n",
    "grid['lat'] = lats\n",
    "grid['lon'] = lons\n",
    "grid.index = keys\n",
    "for sim in range(nsim):\n",
    "    Z = []\n",
    "    for i in range(pgaZ.shape[1]): #pga at each location\n",
    "        Z.append(pgaZ[sim,i])\n",
    "    Z = np.array(Z)\n",
    "    z_t = NNR(np.array([lons,lats]).T,np.array([X,Y]).T,Z,sample_size = -1, n_neighbors = 4, weight = 'distance2')\n",
    "    grid['pga'] = z_t\n",
    "    gdf = gpd.GeoDataFrame(grid, geometry=gpd.points_from_xy(grid['lon'], grid['lat']),crs = 'EPSG:4326')\n",
    "    if sim == 0:\n",
    "        for i in keys:\n",
    "            pgas[i] = []\n",
    "    for i in keys:\n",
    "        pgas[i].append(gdf['pga'][i])\n",
    "        \n",
    "for i in keys:\n",
    "    pgas[i] = np.array(pgas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pgas).to_csv('./pgas_cpt.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
